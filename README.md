## Machine Learning Security 

**Academic Year 2022-2023**
> The course will start on October 6, 2022. 
> [Teams link.](https://teams.microsoft.com/l/team/19%3aH_NJm6PY9cIXGkZs5jclOdZ8NHA_Ce2Xvalcz1FgWsU1%40thread.tacv2/conversations?groupId=87461e8f-9ff0-41be-be67-2c3a70ca6e9a&tenantId=6bfa74cc-fe34-4d57-97d3-97fd6e0edee1)

**Instructors:** Prof. Battista Biggio

**Teaching Assistants:** Dr. Maura Pintor, Dr. Ambra Demontis

**External Seminars:** Dr. Luca Demetrio, Prof. Fabio Roli

**MSc in Computer Engineering, Cybersecurity and Artificial Intelligence (Univ. Cagliari)**

**National PhD Program in Artificial Intelligence**

**PhD Program in Electronic and Computer Engineering (Univ. Cagliari)**

**GitHub repository for course material:** [https://github.com/unica-mlsec/mlsec](https://github.com/unica-mlsec/mlsec)

**Lectures**
- Tuesday, 15-18, room N_3B (Lidia), building N
- Thursday, 12-14, room I_IC (ex BI), building I 


**Course objectives and outcome**

_Objectives_

The objective of this course is to provide students 
with the fundamental elements of machine learning security in the context of different application domains. 
The main concepts and methods of adversarial machine 
learning are presented, from threat modeling to attacks and defenses, 
as well as basic methods to properly evaluate adversarial robustness 
of a machine learning model against different attacks.
 
_Outcome_

An understanding of fundamental concepts and methods of machine learning security and its applications. 
An ability to analyse and evaluate attacks and defenses in the context of application-specific domains. 
An ability to design and evaluate robust machine learning models with Python and test them on benchmark data sets.

**Course materials**
1. [Introduction to the course](slides/01-MLSec-Course-Introduction.pdf)
2. [Threat modeling and attacks on AI/ML models](slides/02-MLSec-Threat-Modeling.pdf)
3. [Evasion Attacks](slides/03-Evasion-Attacks.pdf)
4. [Adversarial Windows Malware (Adversarial EXEmples) - Guest Lecture by Dr. Luca Demetrio](slides/04-AdvEXE.pdf)
5. Poisoning Attacks



**Papers for the reading group exercise**
1. C. Szegedy et al., [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199), ICLR 2014.
2. B. Biggio et al., [Evasion Attacks against Machine Learning at Test Time](https://arxiv.org/abs/1708.06131), ECML PKDD 2013.
3. A. Athalye et al., [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/abs/1802.00420), ICML 2018.
4. F. Croce and M. Hein, [Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks](https://arxiv.org/abs/2003.01690), ICML 2020.
5. F. Croce et al., [Evaluating the Adversarial Robustness of Adaptive Test-time Defenses](https://arxiv.org/pdf/2202.13711.pdf), ICML 2022.
6. C. Yao et al., [Automated Discovery of Adaptive Attacks on Adversarial Defenses](https://arxiv.org/abs/2102.11860), NeurIPS 2021.
7. B. Biggio et al., [Poisoning Attacks against Support Vector Machines](https://arxiv.org/abs/1206.6389), ICML 2012.
8. A. Shafahi et al., [Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/abs/1804.00792), NeurIPS 2018.
9. T. Gu et al., [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733), NIPS-WS 2017.
10. R. Shokri et al., [Membership Inference Attacks against Machine Learning Models](https://arxiv.org/abs/1610.05820), IEEE Symp. S&P 2017.
11. F. Tramer et al., [Stealing Machine Learning Models via Prediction APIs](https://arxiv.org/abs/1609.02943), USENIX Sec. 2016.


