{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Improving security of a classifier\n",
    "\n",
    "In this notebook we will try to make classifiers more robust to adversarial evasion attacks.\n",
    "First, we define a protocol for assessing robustness of classifiers. Then, in the second part of this tutorial, we use a robust model, trained with a widely-used technique called adversarial training.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/unica-mlsec/mlsec/blob/HEAD/labs/04_defenses.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "## Security Evaluation\n",
    "\n",
    "We could be interested in evaluating the **robustness** of a classifier against increasing values of the maximum perturbation $\\varepsilon$.\n",
    "\n",
    "SecML provides a way to easily produce a **Security Evaluation Curve**, by means of the `CSecEval` class.\n",
    "\n",
    "The `CSecEval` instance will take a `CAttack` as input and will test the classifier using the desired perturbation levels.\n",
    "\n",
    "*Please note that the security evaluation process may take a while (up to a few minutes) depending on the machine the script is run on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import secml\n",
    "except ImportError:\n",
    "    %pip install git+https://github.com/pralab/secml\n",
    "\n",
    "try:\n",
    "    import robustbench\n",
    "except ImportError:\n",
    "    %pip install git+https://github.com/RobustBench/robustbench.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_ts = 20  # number of testing samples\n",
    "\n",
    "from secml.data.loader import CDataLoaderMNIST\n",
    "\n",
    "loader = CDataLoaderMNIST()\n",
    "ts = loader.load('testing', num_samples=n_ts)\n",
    "\n",
    "# normalize the data\n",
    "ts.X /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "from secml.ml import CClassifierPyTorch\n",
    "# we will use a pretrained model for this tutorial\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "from robustbench.utils import download_gdrive\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, drop=0.5):\n",
    "        super(SmallCNN, self).__init__()\n",
    "        self.num_channels = 1\n",
    "        self.num_labels = 10\n",
    "        activ = nn.ReLU(True)\n",
    "        self.feature_extractor = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(self.num_channels, 32, 3)),\n",
    "            ('relu1', activ),\n",
    "            ('conv2', nn.Conv2d(32, 32, 3)),\n",
    "            ('relu2', activ),\n",
    "            ('maxpool1', nn.MaxPool2d(2, 2)),\n",
    "            ('conv3', nn.Conv2d(32, 64, 3)),\n",
    "            ('relu3', activ),\n",
    "            ('conv4', nn.Conv2d(64, 64, 3)),\n",
    "            ('relu4', activ),\n",
    "            ('maxpool2', nn.MaxPool2d(2, 2)),\n",
    "        ]))\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(64 * 4 * 4, 200)),\n",
    "            ('relu1', activ),\n",
    "            ('drop', nn.Dropout(drop)),\n",
    "            ('fc2', nn.Linear(200, 200)),\n",
    "            ('relu2', activ),\n",
    "            ('fc3', nn.Linear(200, self.num_labels)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        features = self.feature_extractor(input)\n",
    "        logits = self.classifier(features.view(-1, 64 * 4 * 4))\n",
    "        return logits\n",
    "\n",
    "PRETRAINED_FOLDER = 'pretrained'\n",
    "# create folder for storing models\n",
    "if not os.path.exists(PRETRAINED_FOLDER):\n",
    "    os.mkdir(PRETRAINED_FOLDER)\n",
    "\n",
    "MODEL_ID_REGULAR = '12HLUrWgMPF_ApVSsWO4_UHsG9sxdb1VJ'\n",
    "filepath = os.path.join(PRETRAINED_FOLDER, f'mnist_regular.pth')\n",
    "if not os.path.exists(filepath):\n",
    "    # utility function to handle google drive data\n",
    "    download_gdrive(MODEL_ID_REGULAR, filepath)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "regular_mnist_model = SmallCNN()\n",
    "regular_mnist_model.load_state_dict(torch.load(os.path.join(PRETRAINED_FOLDER, \n",
    "                                                            'mnist_regular.pth'), map_location=device))\n",
    "\n",
    "regular_mnist_clf = CClassifierPyTorch(model=regular_mnist_model, pretrained=True, input_shape=(1, 28, 28))\n",
    "\n",
    "metric = CMetricAccuracy()\n",
    "preds = regular_mnist_clf.predict(ts.X)\n",
    "accuracy = metric.performance_score(y_true=ts.Y, y_pred=preds)\n",
    "print(f\"Accuracy on test set: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's define the attack we want to use for security evaluation\n",
    "from secml.adv.attacks.evasion import CFoolboxPGDLinf\n",
    "\n",
    "y_target = None\n",
    "lb, ub = 0.0, 1.0\n",
    "eps = 0.3  # this will be changed by the security evaluation class\n",
    "alpha = 0.05\n",
    "steps = 100\n",
    "\n",
    "attack = CFoolboxPGDLinf(regular_mnist_clf, y_target,\n",
    "                         lb=lb, ub=ub,\n",
    "                         epsilons=eps,\n",
    "                         abs_stepsize=alpha,\n",
    "                         steps=steps,\n",
    "                         random_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from secml.array import CArray\n",
    "from secml.adv.seceval import CSecEval\n",
    "\n",
    "# TODO run security evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from secml.figure import CFigure\n",
    "\n",
    "fig = CFigure(height=5, width=10)\n",
    "\n",
    "# TODO plot security evaluation\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see how this classifier is *vulnerable* to adversarial attacks, and how we are able to evade it even with small perturbations.\n",
    "\n",
    "In the next part of the tutorial we will try to find a model that is more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Adversarial Training\n",
    "\n",
    "Adversarial training aims at solving a min-max optimization problem. \n",
    "\n",
    "$$\n",
    "\\min _\\theta \\rho(\\theta), \\quad \\text { where } \\quad \\rho(\\theta)=\\mathbb{E}_{(x, y) \\sim \\mathcal{D}}\\left[\\max _{\\delta \\in \\mathcal{S}} L(x+\\delta, y, \\theta)\\right]\n",
    "$$\n",
    "\n",
    "Where we want to solve the inner problem by creating adversarial examples, and the outer problem by feeding these to the training loss.\n",
    "\n",
    "In simpler words, to perform adversarial training we compute adversarial examples and use them as training data for the classifier.\n",
    "\n",
    "REMEMBER: It takes a longer time to train an AT model, because it also has to compute the adversarial examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL_ID_ROBUST = '1gg7Zyly9hcrxtuDfacHXDubg0O1ddGOC'\n",
    "filepath = os.path.join(PRETRAINED_FOLDER, f'mnist_robust_dnn.pth')\n",
    "if not os.path.exists(filepath):\n",
    "    # utility function to handle google drive data\n",
    "    download_gdrive(MODEL_ID_ROBUST, filepath)\n",
    "\n",
    "robust_net = SmallCNN()\n",
    "robust_net.load_state_dict(torch.load(\n",
    "    os.path.join(PRETRAINED_FOLDER, \"mnist_robust_ddn.pth\"), map_location=device))\n",
    "\n",
    "# wrap torch model in CClassifierPyTorch class\n",
    "robust_clf = CClassifierPyTorch(model=robust_net,\n",
    "                                input_shape=(1, 28, 28),\n",
    "                                pretrained=True)\n",
    "\n",
    "y_pred = robust_clf.predict(ts.X)\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "print(\"Accuracy on test set: {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's evaluate again the security of this new robust classifier. Of course, we have to compute again the attacks, as the gradients will have changed after retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO create an attack with the NEW target clf\n",
    "attack_robust = ...\n",
    "\n",
    "# TODO run the security evaluation\n",
    "\n",
    "sec_eval_robust = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from secml.figure import CFigure\n",
    "\n",
    "fig = CFigure(height=5, width=10)\n",
    "\n",
    "# TODO let's plot and compare the two curves\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's define a convenience function to easily plot the MNIST dataset\n",
    "def show_digits(samples, preds, labels, n_display=8, title=None):\n",
    "    digits = list(range(10))\n",
    "    samples = samples.atleast_2d()\n",
    "    n_display = min(n_display, samples.shape[0])\n",
    "    fig = CFigure(width=n_display * 2, height=4)\n",
    "    for idx in range(n_display):\n",
    "        fig.subplot(2, n_display, idx + 1)\n",
    "        fig.sp.xticks([])\n",
    "        fig.sp.yticks([])\n",
    "        fig.sp.imshow(samples[idx, :].reshape((28, 28)), cmap='gray')\n",
    "        fig.sp.title(\"{} ({})\".format(digits[labels[idx].item()], digits[preds[idx].item()]),\n",
    "                     color=(\"green\" if labels[idx].item() == preds[idx].item() else \"red\"))\n",
    "    if title is not None:\n",
    "        fig.title(title)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# take a subset of samples\n",
    "samples = ts[:10, :]\n",
    "\n",
    "# set the attacks epsilons to a desired maximum perturbation\n",
    "attack.epsilon = 0.2\n",
    "attack_robust.epsilon = 0.2\n",
    "\n",
    "y_pred_not_robust, _, adv_ds_not_robust, _ = attack.run(samples.X, samples.Y)\n",
    "y_pred_robust, _, adv_ds_robust, _ = attack_robust.run(samples.X, samples.Y)\n",
    "\n",
    "show_digits(adv_ds_not_robust.X, y_pred_not_robust, samples.Y, n_display=8, title=\"DNN predictions\")\n",
    "show_digits(adv_ds_robust.X, y_pred_robust, samples.Y, n_display=8, title=\"Robust DNN predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "For this first exercise, we are going to test the [transferability](https://www.usenix.org/conference/usenixsecurity19/presentation/demontis) of the adversarial examples.\n",
    "Namely, we are going to test the adversarial examples created against one classifier on a second classifier.\n",
    "Use the results of the previous cell as starting point (we already created the adversarial examples that we need for this step).\n",
    "\n",
    "1. Compute the accuracy of the robust classifier on the adversarial examples created with the standard classifier.\n",
    "2. Compute the accuracy of the standard classifier on the adversarial examples created with the robust classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Compute the security evaluation curve of different sklearn classifiers on a random blob dataset.\n",
    "1. Create a random blob dataset.\n",
    "2. Create two different classifiers.\n",
    "3. Train the classifiers on the dataset, and test the accuracy.\n",
    "4. Compute the two security evaluation curves and show them in a single plot.\n",
    "5. (extra) Try to write a function to compute the security evaluation curve without using the utility from SecML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO write your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('secml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6829dbcfe73f7e6ba320fd39e7c4bddd23e92d1a15475ecfb0305a1647487c5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
