{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf93375",
   "metadata": {},
   "source": [
    "# Defeating Noisy Gradients: Using PGD with Expectation over Transformation (EoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936ce97",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unica-mlsec/mlsec/blob/main/labs/05_defeating_noisy_gradients.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c98b26",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to test the robustness of a defense that was specifically designed to resist gradient-based attacks.\n",
    "\n",
    "Despite the efforts, however, this defense was later shown to be just *hiding* the adversarial examples rather than removing them. It was broken by an **adaptive** attack, *i.e.*, attacks that target the specific defense mechanism that is in place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0074bf",
   "metadata": {},
   "source": [
    "## The k-Winners-Take-All (k-WTA) Defense\n",
    "\n",
    "This defense was proposed in Xiao et al. (2020). This defense replaces the usual activation layer of the DNNs (ReLU) with a k-WTA activation function.\n",
    "\n",
    "As shown in the picture, the ReLU sets to zero the neurons that have a negative output, whereas the k-WTA sets to zero all weights except the top-k (optionally in absolute value, there are different possible configurations).\n",
    "\n",
    "<img style=\"max-width: 300px; height: auto; \" src=\"./assets/kWTA.png\" />\n",
    "\n",
    "\n",
    "This trick aims to break gradient descent by introducing $C_0$ discontinuities on the loss landscape. In this way, any gradient-based attack would fail in finding a consistent direction to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import secmlt\n",
    "except ImportError:\n",
    "    %pip install secml-torch[foolbox]\n",
    "    \n",
    "try:\n",
    "    import robustbench\n",
    "except ImportError:\n",
    "    %pip install git+https://github.com/RobustBench/robustbench.git\n",
    "\n",
    "!git clone https://github.com/unica-mlsec/mlsec\n",
    "%cd mlsec/labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b592093",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from secmlt.models.base_model import BaseModel\n",
    "import torch\n",
    "import torchvision\n",
    "from robustbench.utils import download_gdrive\n",
    "from secmlt.adv.backends import Backends\n",
    "from secmlt.adv.evasion.perturbation_models import LpPerturbationModels\n",
    "from secmlt.adv.evasion.pgd import PGD, PGDNative\n",
    "from secmlt.metrics.classification import Accuracy\n",
    "from secmlt.models.pytorch.base_pytorch_nn import BasePytorchClassifier\n",
    "from secmlt.trackers import LossTracker\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from utils.models import SparseResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a54091",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = Path(\"models\")\n",
    "if not pretrained_path.exists():\n",
    "    pretrained_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# key of a Google Drive file containing the pretrained model\n",
    "MODEL_ID = \"1Af_owmMvg1LxjITLE1gFUmPx5idogeTP\"\n",
    "\n",
    "gamma = 0.1  # sparsity ratio\n",
    "filepath = pretrained_path / f\"kwta_spresnet18_{gamma}_cifar_adv.pth\"\n",
    "if not filepath.exists():\n",
    "    # utility function to handle google drive data\n",
    "    download_gdrive(MODEL_ID, filepath)\n",
    "\n",
    "# check out the model class in the imported module to see the implementation\n",
    "# of this defense\n",
    "model = SparseResNet18(sparsities=[gamma, gamma, gamma, gamma])\n",
    "\n",
    "# check if CUDA is available\n",
    "if not torch.cuda.is_available():\n",
    "    state_dict = torch.load(filepath, map_location=\"cpu\")\n",
    "else:\n",
    "    state_dict = torch.load(filepath)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "dataset_path = \"data/datasets/\"  # relative to this notebook's folder\n",
    "\n",
    "cifar_10 = torchvision.datasets.CIFAR10(\n",
    "    root=dataset_path, train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "cifar_10_subset = Subset(cifar_10, list(range(5)))\n",
    "cifar_10_data_loader = DataLoader(cifar_10_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "classifier = BasePytorchClassifier(model)\n",
    "print(\"Test accuracy: \", Accuracy()(classifier, cifar_10_data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805a913",
   "metadata": {},
   "source": [
    "We first attack the model with untargeted PGD $\\ell_\\infty$, $\\varepsilon=0.03$, $\\alpha=0.005$ and 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1dcae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "epsilon = 8 / 255\n",
    "step_size = 0.005\n",
    "lb = 0.0\n",
    "ub = 1.0\n",
    "random_start = False\n",
    "y_target = None\n",
    "perturbation_model = LpPerturbationModels.LINF\n",
    "\n",
    "pgd_foolbox = PGD(\n",
    "    epsilon=epsilon,\n",
    "    num_steps=num_steps,\n",
    "    step_size=step_size,\n",
    "    perturbation_model=perturbation_model,\n",
    "    backend=Backends.FOOLBOX,\n",
    "    y_target=y_target,\n",
    "    lb=lb,\n",
    "    ub=ub,\n",
    "    random_start=random_start,\n",
    ")\n",
    "adv_ds_fb = pgd_foolbox(classifier, cifar_10_data_loader)\n",
    "print(\"Robust accuracy (PGD Foolbox): \", Accuracy()(classifier, adv_ds_fb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf57c9",
   "metadata": {},
   "source": [
    "First, attacking this model results in optimizing over a very noisy loss function. In fact, we are not able to see this with the Foolbox attack, but we can inspect the loss with our native implementation. Moreover, our native implementation takes the best point instead of the last, therefore avoiding silent failures (Pintor, 2022).\n",
    "This might result in a better estimation already, as our attack tracks whether any of the iterates inside the optimization loop are succesfully adversarial, keeping the best points found.\n",
    "\n",
    "<img style=\"max-width: 300px; height: auto; \" src=\"./assets/best_point.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80309b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd = PGD(\n",
    "    epsilon=epsilon,\n",
    "    num_steps=num_steps,\n",
    "    step_size=step_size,\n",
    "    perturbation_model=perturbation_model,\n",
    "    backend=Backends.NATIVE,\n",
    "    trackers=[LossTracker()],\n",
    "    y_target=y_target,\n",
    "    lb=lb,\n",
    "    ub=ub,\n",
    "    random_start=random_start,\n",
    ")\n",
    "adv_ds = pgd(classifier, cifar_10_data_loader)\n",
    "print(\"Robust accuracy (PGD Native): \", Accuracy()(classifier, adv_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27315ed4",
   "metadata": {},
   "source": [
    "Did it work?\n",
    "Probably not as expected... But the model is not as robust as it seems.\n",
    "Let's try to visualize the loss of the attack on one single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f968c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get loss\n",
    "loss_pgd = pgd.trackers[0].get()       # shape [num_samples, num_steps]\n",
    "\n",
    "num_samples = loss_pgd.shape[0]\n",
    "\n",
    "# create subplots (one row, multiple columns)\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=num_samples,\n",
    "    figsize=(5 * num_samples, 5),\n",
    ")\n",
    "\n",
    "if num_samples == 1:\n",
    "    axes = [axes]  # ensure iterable if single sample\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(loss_pgd[i], linestyle=\"-\", alpha=0.9, label=\"PGD\")\n",
    "    ax.set_title(f\"Sample {i}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "fig.suptitle(\"Per-sample loss curves: PGD\")\n",
    "\n",
    "# get only one legend for the entire figure\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.95, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16241595",
   "metadata": {},
   "source": [
    "The attack is not working because the optimization is not really improving the objective.\n",
    "This is caused by gradient obfuscation.\n",
    "As explained before, the model is indeed causing the loss landscape to become extremely noisy (this is shown also in the paper).\n",
    "\n",
    "![landscape](./assets/kWTA_landscape.png)\n",
    "\n",
    "How to fix this? We need an **adaptive attack** (Tramer, 2020).\n",
    "\n",
    "Open the referenced paper and find the model that we just attacked and read through the section (hint: go to https://arxiv.org and find the paper there).\n",
    "What causes the model to be strong against gradient-based attacks?\n",
    "How did the authors of Tramer et al. (2020) break the defense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3e439",
   "metadata": {},
   "source": [
    "## Adaptive attack for k-WTA\n",
    "\n",
    "We are now using an implementation of the attack in Tramer et al. (2020) (adapted for SecML-Torch).\n",
    "\n",
    "The attack estimates the gradient by querying the model (without computing the white-box gradient), and by computing a finite-difference approximation on sets of points sampled in a neighborhood of the sample.\n",
    "The smoother approximation is then obtained by averaging the direction of all these estimated gradients.\n",
    "\n",
    "This approximation has a cost: it has to query the model multiple time for each step of the attack, as the gradient has to be estimated locally each time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fc7ff",
   "metadata": {},
   "source": [
    "Note: the following lines take a long time to complete the attack. For this reason, they are commented. Feel free to uncomment to check how the attack runs and finds better results than the baseline PGD. \n",
    "\n",
    "We will load the plot directly to display the results while avoiding recompiling in our docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from secmlt.adv.evasion.modular_attacks.eot_gradient import EoTGradientMixin\n",
    "\n",
    "# re-initialize trackers\n",
    "trackers = [LossTracker()]\n",
    "\n",
    "\n",
    "# custom pgd with EoT\n",
    "class PGDEoT(EoTGradientMixin,PGDNative):\n",
    "    ...  # inherits everything from PGDNative and EoTGradientMixin\n",
    "    \n",
    "\n",
    "\n",
    "pgd_eot = PGDEoT(\n",
    "    eot_samples=300,\n",
    "    eot_radius=0.03,\n",
    "    epsilon=epsilon,\n",
    "    num_steps=num_steps,\n",
    "    step_size=step_size,\n",
    "    perturbation_model=perturbation_model,\n",
    "    backend=Backends.NATIVE,\n",
    "    trackers=[LossTracker()],\n",
    "    y_target=y_target,\n",
    "    lb=lb,\n",
    "    ub=ub,\n",
    "    random_start=random_start,\n",
    ")\n",
    "\n",
    "# uncomment for running the attack (it takes a long time)\n",
    "# adv_ds_eot = pgd_eot(classifier, cifar_10_data_loader)\n",
    "# print(\"Robust accuracy (PGD EoT): \", Accuracy()(classifier, adv_ds_eot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a94798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to plot the losses after running the attack\n",
    "\n",
    "# # get loss\n",
    "# loss_pgd = pgd.trackers[0].get()       # shape [num_samples, num_steps]\n",
    "# loss_eot = pgd_eot.trackers[0].get()   # same shape\n",
    "\n",
    "# num_samples = loss_pgd.shape[0]\n",
    "\n",
    "# # create subplots (one row, multiple columns)\n",
    "# fig, axes = plt.subplots(\n",
    "#     nrows=1,\n",
    "#     ncols=num_samples,\n",
    "#     figsize=(5 * num_samples, 5),\n",
    "# )\n",
    "\n",
    "# if num_samples == 1:\n",
    "#     axes = [axes]  # ensure iterable if single sample\n",
    "\n",
    "# for i, ax in enumerate(axes):\n",
    "#     ax.plot(loss_pgd[i], linestyle=\"-\", alpha=0.9, label=\"PGD\")\n",
    "#     ax.plot(loss_eot[i], linestyle=\"--\", alpha=0.9, label=\"PGD EoT\")\n",
    "#     ax.set_title(f\"Sample {i}\")\n",
    "#     ax.grid(alpha=0.3)\n",
    "\n",
    "# axes[0].set_ylabel(\"Loss\")\n",
    "# for ax in axes:\n",
    "#     ax.set_xlabel(\"Iteration\")\n",
    "\n",
    "# fig.suptitle(\"Per-sample loss curves: PGD vs PGD EoT\")\n",
    "\n",
    "# # get only one legend for the entire figure\n",
    "# handles, labels = axes[0].get_legend_handles_labels()\n",
    "# fig.legend(handles, labels, loc=\"upper right\")\n",
    "\n",
    "# plt.tight_layout(rect=[0, 0, 0.95, 0.95])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94164988",
   "metadata": {},
   "source": [
    "```text\n",
    "Robust accuracy (PGD EoT):  tensor(0.2000)\n",
    "```\n",
    "\n",
    "![pgd-vs-eot](./assets/adaptive_attacks_loss_curves.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e6059",
   "metadata": {},
   "source": [
    "## References\n",
    "* Maura Pintor, Luca Demetrio, Angelo Sotgiu, Ambra Demontis, Nicholas Carlini, Battista Biggio, and Fabio Roli. Indicators of attack failure: debugging and improving optimization of adversarial examples. Advances in Neural Information Processing Systems, 35:23063–23076, 2022.\n",
    "* Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses. Advances in neural information processing systems, 33:1633–1645, 2020.\n",
    "* Chang Xiao, Peilin Zhong, and Changxi Zheng. Enhancing adversarial defense by k-winners-take-all. In International Conference on Learning Representations. 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secmltorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
